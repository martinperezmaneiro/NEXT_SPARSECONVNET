{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sparseconvnet as scn\n",
    "from next_sparseconvnet.utils.data_loaders import DataGen, collatefn, LabelType\n",
    "from next_sparseconvnet.networks.architectures import UNet\n",
    "from next_sparseconvnet.utils.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer una funcion que me calcule el IoU para un numero n de clases..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.random.randint(0, 3, 50000)\n",
    "pred = np.random.randint(0, 3, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(true, pred, nclass = 3):\n",
    "    \"\"\"\n",
    "        Intersection over union is a metric for semantic segmentation. \n",
    "        It returns a IoU value for each class of our input tensors/arrays.\n",
    "    \"\"\"\n",
    "    confusion_matrix = np.zeros((nclass, nclass))\n",
    "\n",
    "    for i in range(len(true)):\n",
    "        confusion_matrix[true[i]][pred[i]] += 1\n",
    "    \n",
    "    IoU = []\n",
    "    for i in range(nclass): \n",
    "        IoU.append(confusion_matrix[i, i]/(sum(confusion_matrix[:, i]) + sum(confusion_matrix[i, :]) - confusion_matrix[i, i]))\n",
    "    return np.array(IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.08964253 4.98856122 5.06654512]\n"
     ]
    }
   ],
   "source": [
    "a = IoU(true, pred) #lo hace bastante rapido para arrays grandes\n",
    "b = np.array([1, 1, 1])\n",
    "print(b / a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19647745 0.2004586  0.19737316]\n"
     ]
    }
   ],
   "source": [
    "print(np.zeros(3)+ a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que IoU me hace bien la confusion matrix con tensores de entrada en lugar de arrays, bien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defino una funcion que entrena un epoch en el que le tengo que decir que numero de epoch es, el tamaño del batch que se pasa, la red, loss, optimizer y el loader con los datos\n",
    "\n",
    "Creo que tengo q quitar batch_size como parametro y poner simplemente batch_size como la len de event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_id, net, criterion, optimizer, loader): \n",
    "    \"\"\"       \n",
    "        Trains the net for all the train data one time\n",
    "    \"\"\"\n",
    "    net.train()\n",
    "    loss_epoch, iou_epoch = [], []\n",
    "    for batchid, (coord, ener, label, event) in enumerate(loader):\n",
    "        label = label.type(torch.LongTensor) #quitar esto una vez se corrija en el collate\n",
    "        batch_size = len(event)\n",
    "        ener, label = ener.cuda(), label.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        output = net.forward((coord, ener, batch_size))\n",
    "            \n",
    "        loss = criterion(output, label) \n",
    "        loss.backward()\n",
    "            \n",
    "        optimizer.step()\n",
    "            \n",
    "        loss_epoch.append(loss.item())\n",
    "            \n",
    "        #IoU\n",
    "        softmax = torch.nn.Softmax(dim = 1)\n",
    "        prediction = torch.argmax(softmax(output), 1) \n",
    "        iou_epoch.append(IoU(label.cpu(), prediction.cpu()))\n",
    "        \n",
    "        if batchid%2==0:\n",
    "            progress = f\"Train Epoch: {epoch_id} [{batchid*batch_size:5}/{len(loader.dataset)}\" +\\\n",
    "            f\" ({int(100*batchid/len(loader)):2}%)]\"\n",
    "            loss_ = f\"\\t Loss: {loss.item():.6f}\"\n",
    "            print(progress + loss_)\n",
    "                \n",
    "    return loss_epoch, iou_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch(net, loader):\n",
    "    \"\"\"\n",
    "        Computes loss and IoU for all the validation data\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    loss_epoch, iou_epoch = [], []\n",
    "    with torch.autograd.no_grad():\n",
    "        for batchid, (coord, ener, label, event) in enumerate(loader):\n",
    "            batch_size = len(event)\n",
    "            ener, label = ener.cuda(), label.cuda()\n",
    "                    \n",
    "            output = net.forward((coord, ener, batch_size))\n",
    "            \n",
    "            loss = criterion(output, label) \n",
    "            \n",
    "            loss_epoch.append(loss.item())\n",
    "            \n",
    "            #IoU\n",
    "            softmax = torch.nn.Softmax(dim = 1)\n",
    "            prediction = torch.argmax(softmax(output), 1) \n",
    "            iou_epoch.append(IoU(label.cpu(), prediction.cpu()))\n",
    "    return loss_epoch, iou_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo los loaders con datos de train y validation. En principio unos pocos para probar (entendi sobre 100 de train...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/home/mmkekic/MC_dataset/new_data/train_dataset_200.h5\"\n",
    "valid_path = \"/home/mmkekic/MC_dataset/new_data/valid_dataset_10.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevents_train = 100 #numero de eventos que pillo de cada dataset\n",
    "nevents_valid = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train = DataGen(train_path, LabelType.Segmentation, nevents = nevents_train)\n",
    "datagen_valid = DataGen(valid_path, LabelType.Segmentation, nevents = nevents_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 10\n",
    "batch_size_valid = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = torch.utils.data.DataLoader(datagen_train, batch_size = batch_size_train, shuffle = True, num_workers=1, collate_fn=collatefn, drop_last=True, pin_memory=False)\n",
    "loader_valid = torch.utils.data.DataLoader(datagen_valid, batch_size = batch_size_valid, shuffle = True, num_workers=1, collate_fn=collatefn, drop_last=True, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch_segmentation(net, criterion, loader, nclass = 3):\n",
    "    \"\"\"\n",
    "        Computes loss and IoU for all the validation data\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    loss_epoch, iou_epoch = 0, np.zeros(nclass)\n",
    "    with torch.autograd.no_grad():\n",
    "        for batchid, (coord, ener, label, event) in enumerate(loader):\n",
    "            batch_size = len(event)\n",
    "            ener, label = ener.cuda(), label.cuda()\n",
    "\n",
    "            output = net.forward((coord, ener, batch_size))\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            loss_epoch += loss.item()\n",
    "            print(loss_epoch, len(loader))\n",
    "            #IoU\n",
    "            softmax = torch.nn.Softmax(dim = 1)\n",
    "            prediction = torch.argmax(softmax(output), 1)\n",
    "            iou_epoch += IoU(label.cpu(), prediction.cpu())\n",
    "\n",
    "        loss_epoch = loss_epoch / len(loader)\n",
    "        iou_epoch = iou_epoch / len(loader)\n",
    "        loss_ = f\"\\t Validation Loss: {loss_epoch:.6f}\"\n",
    "        print(loss_)\n",
    "\n",
    "    return loss_epoch, iou_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\t Loss: 0.835661\n",
      "\t Validation Loss: 1662949792.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1662949792.0, array([0.0164881 , 0.82769225, 0.01143791]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = UNet((561, 561, 561), 6, 3, [9, 9, 3], [2, 2], 3) \n",
    "net = net.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, betas=(0.9, 0.999), eps=1e-6, weight_decay=0)\n",
    "\n",
    "train_one_epoch_segmentation(0, net, criterion, optimizer, loader_train)\n",
    "valid_one_epoch_segmentation(net, criterion, loader_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos a sacar las clases que me predice la net y pruebo tambien a ver como funciona IoU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [datagen_train[0]]\n",
    "coord, ener, label, event = collatefn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet((561, 561, 561), 6, 3, [9, 9, 7, 5, 3, 3, 3], 2, 3) \n",
    "output = net.forward((coord, ener))\n",
    "del(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "t = torch.argmax(softmax(output), 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.008264462809917356, 0.502092050209205, 0.08333333333333333]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IoU(label, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que va bien. Aun asi, cuando lo haga tendre que PASAR TENSORES A CPU EN FUNCION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-6ad945cb6633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmem_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmem_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_params\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmem_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "mem_params = sum([param.nelement()*param.element_size() for param in net.parameters()])\n",
    "mem_bufs = sum([buf.nelement()*buf.element_size() for buf in net.buffers()])\n",
    "print((mem_params+mem_bufs)/1024**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asigno la red que voy a usar. Se procuran parámetros con spatial size mayor que el del detector, que es 400x400x530 (tenia esto apuntado pero creo que esta mal pq mirando ahora pone que son 441x441x550...), de forma que en la bottom layer la imagen llegue a tener dimension 7x7x7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = UNet((561, 561, 561), 6, 3, [9, 9, 7, 5, 3, 3, 3], 2, 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss y optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = torch.nn.CrossEntropyLoss() \n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, betas=(0.9, 0.999), eps=1e-6, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Entreno a ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from next_sparseconvnet.utils.train_utils import *\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = UNet((561, 561, 561), 6, 3, [9, 9, 7, 5, 3, 3, 3], [2, 2, 2, 2, 2, 2], 3) \n",
    "#save_checkpoint(net.state_dict(), 'checkpoint_try')\n",
    "#del(net)\n",
    "#red = net.load_state_dict(torch.load('checkpoint_try'))\n",
    "#vuelve a ser la red entiendo, da igual el nombre q le ponga al archivo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_segmentation(*, nepoch, train_data_path, valid_data_path, train_batch_size, valid_batch_size, net, criterion, optimizer, checkpoint_dir, tensorboard_dir):\n",
    "    train_gen = DataGen(train_data_path, LabelType.Segmentation, nevents = 100)\n",
    "    valid_gen = DataGen(valid_data_path, LabelType.Segmentation, nevents = 10)\n",
    "    \n",
    "    loader_train = torch.utils.data.DataLoader(train_gen, batch_size = train_batch_size, shuffle = True, num_workers=1, collate_fn=collatefn, drop_last=True, pin_memory=False)\n",
    "    loader_valid = torch.utils.data.DataLoader(valid_gen, batch_size = valid_batch_size, shuffle = True, num_workers=1, collate_fn=collatefn, drop_last=True, pin_memory=False)\n",
    "    \n",
    "    start_loss = np.inf\n",
    "    writer = SummaryWriter(tensorboard_dir)\n",
    "    for i in range(nepoch):\n",
    "        train_loss, train_iou = train_one_epoch_segmentation(i, net, criterion, optimizer, loader_train)\n",
    "        valid_loss, valid_iou = valid_one_epoch_segmentation(net, criterion, loader_valid)\n",
    "        \n",
    "        if valid_loss < start_loss:\n",
    "            save_checkpoint({'state_dict': net.state_dict(),\n",
    "                             'optimizer': optimizer.state_dict()}, f'{checkpoint_dir}/net_checkpoint_{i}.pth.tar') \n",
    "            start_loss = valid_loss\n",
    "        \n",
    "        writer.add_scalar('loss/train', train_loss, i)\n",
    "        for k, iou in enumerate(train_iou):\n",
    "            writer.add_scalar(f'iou/train_{k}class', iou, i)\n",
    "        \n",
    "        writer.add_scalar('loss/valid', valid_loss, i)\n",
    "        for k, iou in enumerate(valid_iou):\n",
    "            writer.add_scalar(f'iou/valid_{k}class', iou, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train params\n",
    "nepoch = 2\n",
    "train_file = '/home/mmkekic/MC_dataset/new_data/train_dataset_all.h5'\n",
    "valid_file = '/home/mmkekic/MC_dataset/new_data/valid_dataset_10.h5'\n",
    "train_batch = 10\n",
    "valid_batch = 2\n",
    "checkpoint_dir = 'home/mperez/NEXT_SPARSECONVNET/scripts/save_progress'\n",
    "tensorboard_dir = 'home/mperez/NEXT_SPARSECONVNET/scripts/save_progress'\n",
    "num_workers = 1\n",
    "nevents_train = 100\n",
    "nevents_valid = 10\n",
    "\n",
    "#UNet params\n",
    "spatial_size      = (543, 543, 543)\n",
    "init_conv_nplanes = 8\n",
    "init_conv_kernel  = 7\n",
    "kernel_sizes      = [7, 7, 5, 3, 3, 3]\n",
    "stride_sizes      = [4, 2, 2, 2, 2]\n",
    "basic_num         = 2\n",
    "\n",
    "#Optimizer parameters\n",
    "lr = 1e-2\n",
    "betas = (0.9, 0.999)\n",
    "eps = 1e-6\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76885.7109375 5\n",
      "96662.001953125 5\n",
      "113494.34375 5\n",
      "172266.40625 5\n",
      "232724.421875 5\n",
      "\t Validation Loss: 46544.884375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46544.884375, array([7.14285714e-03, 8.81238272e-01, 1.16107996e-17]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_one_epoch_segmentation(net, criterion, loader_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\t Loss: 0.708779\n",
      "\t Validation Loss: 80155773.600000\n",
      "Train Epoch: 1\t Loss: 0.515038\n",
      "\t Validation Loss: 4.461229\n"
     ]
    }
   ],
   "source": [
    "net = UNet((543, 543, 543), 8, 7, [7, 7, 5, 3, 3, 3], [4, 2, 2, 2, 2], 2) \n",
    "net = net.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2, betas=(0.9, 0.999), eps=1e-6, weight_decay=0)\n",
    "\n",
    "train_segmentation(nepoch = 2, \n",
    "                   train_data_path = train_path, \n",
    "                   valid_data_path = valid_path, \n",
    "                   train_batch_size = 10,\n",
    "                   valid_batch_size = 2,\n",
    "                   net = net, \n",
    "                   criterion = criterion, \n",
    "                   optimizer = optimizer, \n",
    "                   checkpoint_dir = 'checkpoint_example', \n",
    "                   tensorboard_dir = 'tensorboard_example',\n",
    "                   num_workers = 1,\n",
    "                   nevents_train = 100,\n",
    "                   nevents_valid = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
